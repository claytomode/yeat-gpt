{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2844eff8",
   "metadata": {},
   "source": [
    "# YEAT GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574799eb",
   "metadata": {},
   "source": [
    "## Scrape Genius for Yeat Lyrics\n",
    "\n",
    "Genius is really strict and kept rate limiting me. It was for only a few hours at first and then it was for over a day. This was my solution to avoid that. This script took over a few hours to run.\n",
    "\n",
    "I decided to just save the raw HTML of each webpage. It autosaves and pickles every 5 songs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a45f5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "import os\n",
    "import httpx\n",
    "import asyncio\n",
    "import random\n",
    "import pickle\n",
    "from dataclasses import dataclass\n",
    "from dotenv import load_dotenv\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19bf9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "GENIUS_TOKEN = os.getenv(\"GENIUS_CLIENT_ACCESS_TOKEN\")\n",
    "\n",
    "ARTIST_NAME = \"Yeat\"\n",
    "API_BASE_URL = \"https://api.genius.com\"\n",
    "PROGRESS_FILE = \"yeat_raw_html_progress.pkl\"\n",
    "\n",
    "USER_AGENTS = [\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'\n",
    "]\n",
    "\n",
    "@dataclass\n",
    "class Song:\n",
    "    title: str\n",
    "    url: str\n",
    "    id: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ee2864",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c37dfde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tclay\\Documents\\projects\\yeat-gpt\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming! Found 1280 songs already downloaded.\n",
      "Finding Artist ID for Yeat...\n",
      "Found Artist ID: 1476681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Metadata: 87page [01:24,  1.03page/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All songs are already downloaded!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# we add basic saving and resume logic here because it will take hours to scrape all songs safely\n",
    "scraped_data = []\n",
    "scraped_ids = set()\n",
    "\n",
    "if os.path.exists(PROGRESS_FILE):\n",
    "    with open(PROGRESS_FILE, \"rb\") as f:\n",
    "        scraped_data = pickle.load(f)\n",
    "        scraped_ids = {entry['song'].id for entry in scraped_data}\n",
    "    print(f\"Resuming! Found {len(scraped_data)} songs already downloaded.\")\n",
    "\n",
    "async def fetch_song_list():\n",
    "    headers = {\"Authorization\": f\"Bearer {GENIUS_TOKEN}\", \"User-Agent\": random.choice(USER_AGENTS)}\n",
    "    async with httpx.AsyncClient(headers=headers, timeout=30) as client:\n",
    "        # get artist id\n",
    "        print(f\"Finding Artist ID for {ARTIST_NAME}...\")\n",
    "        search = await client.get(f\"{API_BASE_URL}/search\", params={\"q\": ARTIST_NAME})\n",
    "        if search.status_code != 200: return []\n",
    "        \n",
    "        hits = search.json()['response']['hits']\n",
    "        artist_id = next((h['result']['primary_artist']['id'] for h in hits if h['result']['primary_artist']['name'].lower() == ARTIST_NAME.lower()), None)\n",
    "        \n",
    "        if not artist_id: return []\n",
    "        print(f\"Found Artist ID: {artist_id}\")\n",
    "\n",
    "        # get songs\n",
    "        songs = []\n",
    "        page = 1\n",
    "        with tqdm(desc=\"Fetching Metadata\", unit=\"page\") as pbar:\n",
    "            while page:\n",
    "                await asyncio.sleep(0.5)\n",
    "                res = await client.get(f\"{API_BASE_URL}/artists/{artist_id}/songs\", params={\"per_page\": 50, \"page\": page, \"sort\": \"popularity\"})\n",
    "                if res.status_code != 200: break\n",
    "                \n",
    "                data = res.json()['response']\n",
    "                for s in data['songs']:\n",
    "                    if s['primary_artist']['id'] == artist_id:\n",
    "                        songs.append(Song(s['title'], s['url'], s['id']))\n",
    "                \n",
    "                pbar.update(1)\n",
    "                page = data['next_page']\n",
    "        return songs\n",
    "\n",
    "async def download_safely(songs):\n",
    "    songs_to_download = [s for s in songs if s.id not in scraped_ids]\n",
    "    \n",
    "    if not songs_to_download:\n",
    "        print(\"All songs are already downloaded!\")\n",
    "        return\n",
    "\n",
    "    print(f\"Starting safe download for {len(songs_to_download)} new songs.\")\n",
    "    \n",
    "    async with httpx.AsyncClient(timeout=30, follow_redirects=True) as client:\n",
    "        for song in tqdm(songs_to_download, desc=\"Downloading Lyrics\", unit=\"song\"):\n",
    "            \n",
    "            retries = 3\n",
    "            while retries > 0:\n",
    "                try:\n",
    "                    # 5 to 10 sec pause :/\n",
    "                    await asyncio.sleep(random.uniform(5.0, 10.0))\n",
    "\n",
    "                    headers = {\"User-Agent\": random.choice(USER_AGENTS)}\n",
    "                    res = await client.get(song.url, headers=headers)\n",
    "\n",
    "                    # handle blocks\n",
    "                    if res.status_code in [403, 429, 1015]:\n",
    "                        tqdm.write(f\"RATE LIMIT HIT. Pausing 2 mins... ({song.title})\")\n",
    "                        await asyncio.sleep(120) \n",
    "                        retries -= 1\n",
    "                        continue \n",
    "                    \n",
    "                    if res.status_code == 200:\n",
    "                        entry = {\"song\": song, \"html\": res.text}\n",
    "                        scraped_data.append(entry)\n",
    "                        \n",
    "                        # autosave every 5 songs\n",
    "                        if len(scraped_data) % 5 == 0:\n",
    "                            with open(PROGRESS_FILE, \"wb\") as f:\n",
    "                                pickle.dump(scraped_data, f)\n",
    "                        break \n",
    "                    \n",
    "                except Exception as e:\n",
    "                    tqdm.write(f\"Error: {e}\")\n",
    "                    await asyncio.sleep(5)\n",
    "                \n",
    "                retries -= 1\n",
    "    \n",
    "    with open(PROGRESS_FILE, \"wb\") as f:\n",
    "        pickle.dump(scraped_data, f)\n",
    "    print(f\"\\nSaved {len(scraped_data)} songs to {PROGRESS_FILE}\")\n",
    "\n",
    "async def main():\n",
    "    all_songs = await fetch_song_list()\n",
    "    if all_songs:\n",
    "        await download_safely(all_songs)\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ecfa73",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "Now we use XPath selectors to find the lyrics and then clean to only include Yeat's raw lyrics.\n",
    "\n",
    "Genius doesn't do a great job designated featured verses, producer tags, adlibs, etc. so the data isn't going to be that clean. That is okay for a project like this :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdd4fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! Saved 970 songs.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import re\n",
    "from lxml import html\n",
    "\n",
    "with open(\"yeat_raw_html_progress.pkl\", \"rb\") as f:\n",
    "    scraped_data = pickle.load(f)\n",
    "\n",
    "clean_songs = []\n",
    "\n",
    "def process_lyrics_with_styles(lyric_containers):\n",
    "    final_lyrics = []\n",
    "    \n",
    "    for container in lyric_containers:\n",
    "        current_style_needed = None \n",
    "        \n",
    "        for element in container.xpath(\"./* | ./text()\"):\n",
    "            text_val = element if isinstance(element, str) else element.text_content()\n",
    "            \n",
    "            header_match = re.search(r'\\[(.*?):(.*?)]', text_val)\n",
    "            if header_match:\n",
    "                header_content = header_match.group(0).lower()\n",
    "                if \"yeat\" in header_content:\n",
    "                    if \"italic\" in header_content:\n",
    "                        current_style_needed = \"i\"\n",
    "                    elif \"bold\" in header_content:\n",
    "                        current_style_needed = \"b\"\n",
    "                    else:\n",
    "                        current_style_needed = None \n",
    "                else:\n",
    "                    current_style_needed = \"SKIP\"\n",
    "                continue\n",
    "\n",
    "            if current_style_needed == \"SKIP\":\n",
    "                continue\n",
    "\n",
    "            if isinstance(element, str):\n",
    "                if current_style_needed is None:\n",
    "                    final_lyrics.append(element)\n",
    "            else:\n",
    "                tag = element.tag\n",
    "                if current_style_needed is None:\n",
    "                    final_lyrics.append(element.text_content())\n",
    "                elif tag == current_style_needed:\n",
    "                    final_lyrics.append(element.text_content())\n",
    "                if tag == \"br\":\n",
    "                    final_lyrics.append(\"\\n\")\n",
    "\n",
    "    return \"\".join(final_lyrics)\n",
    "\n",
    "for entry in scraped_data:\n",
    "    raw_html = entry['html']\n",
    "    tree = html.fromstring(raw_html)\n",
    "    lyric_containers = tree.xpath('//div[@data-lyrics-container=\"true\"]')\n",
    "    \n",
    "    if lyric_containers:\n",
    "        yeat_only_text = process_lyrics_with_styles(lyric_containers)\n",
    "        \n",
    "        # kill everything from \"Read More\" onwards (metadata/descriptions)\n",
    "        # This catches the \"The snippet of this track... Read More\" blocks\n",
    "        yeat_only_text = re.sub(r'.*?Read More.*', '', yeat_only_text, flags=re.DOTALL)\n",
    "        \n",
    "        # kill the \"Embed\" text and trailing digits that Genius adds at the bottom\n",
    "        yeat_only_text = re.sub(r'\\d*Embed$', '', yeat_only_text.strip())\n",
    "        yeat_only_text = re.sub(r'\\d+Embed', '', yeat_only_text)\n",
    "\n",
    "        # this is from features that are missed or producer tags. we do not want this in the training data!!!!\n",
    "        n_word_pattern = r'\\bn[i|e]gg[a|e][rh]?s?\\b'\n",
    "        yeat_only_text = re.sub(n_word_pattern, \"[SCRUBBED]\", yeat_only_text, flags=re.IGNORECASE)\n",
    "        \n",
    "        # remove brackets [Chorus], [Verse], etc. \n",
    "        yeat_only_text = re.sub(r'\\[.*?\\]', '', yeat_only_text)\n",
    "        \n",
    "        # remove parentheses but keep content\n",
    "        yeat_only_text = yeat_only_text.replace(\"(\", \"\").replace(\")\", \"\")\n",
    "\n",
    "        # fix cases where the break tag was misplaced and there isn't a space\n",
    "        yeat_only_text = re.sub(r'([a-z0-9\\!\\?\\'\\\"]) ([A-Z])', r'\\1 \\2', yeat_only_text)\n",
    "        \n",
    "        lines = []\n",
    "        for line in yeat_only_text.splitlines():\n",
    "            clean_line = line.strip()\n",
    "\n",
    "            # filter out Genius UI clutter\n",
    "            noise_words = [\"You might also like\", \"Contributors\", \"Lyrics\", \"Songs Like\", \"Translations\"]\n",
    "            if clean_line and not any(x in clean_line for x in noise_words):\n",
    "                lines.append(clean_line)\n",
    "        \n",
    "        if lines:\n",
    "            clean_songs.append(\"\\n\".join(lines))\n",
    "\n",
    "\n",
    "if clean_songs:\n",
    "    output_file = \"yeat_lyrics_clean.txt\"\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        content = \"\\n\\n<|endoftext|>\\n\\n\".join(clean_songs)\n",
    "        f.write(content)\n",
    "    print(f\"Success! Saved {len(clean_songs)} songs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50598b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA status: True\n",
      "Active GPU: NVIDIA GeForce RTX 3070\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f\"CUDA status: {torch.cuda.is_available()}\")\n",
    "print(f\"Active GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cb075c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tclay\\Documents\\projects\\yeat-gpt\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training starting... (Logs will only appear every 100 steps)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3669' max='3669' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3669/3669 19:52, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.887300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>3.530200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>3.382200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>3.331100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.302800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>3.194300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>3.157800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>3.159200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>3.134600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>3.045500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>3.051700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>3.044900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>2.920900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>2.894300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.861000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>2.848100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>2.834800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>2.808800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>2.839000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.785600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>2.804500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>2.794300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>2.811300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>2.772000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>2.715100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>2.696100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>2.678900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>2.676500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>2.665800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>2.659400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>2.646500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>2.659600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>2.653200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>2.654500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>2.647200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>2.657100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to 'yeat-gpt-final'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from datasets import load_dataset\n",
    "\n",
    "# setup\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)\n",
    "\n",
    "dataset = load_dataset('text', data_files='yeat_lyrics_clean.txt')\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"yeat-gpt-v1\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,   \n",
    "    gradient_accumulation_steps=2,    \n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    fp16=True,                       \n",
    "    dataloader_num_workers=0,         \n",
    "    \n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=100,\n",
    "    disable_tqdm=False,\n",
    "    report_to=\"none\",\n",
    "    \n",
    "    save_steps=500,\n",
    "    save_total_limit=1\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"Training starting... (Logs will only appear every 100 steps)\")\n",
    "trainer.train()\n",
    "\n",
    "model.save_pretrained(\"yeat-gpt-final\")\n",
    "tokenizer.save_pretrained(\"yeat-gpt-final\")\n",
    "print(\"Model saved to 'yeat-gpt-final'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9a5c50f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tclay\\Documents\\projects\\yeat-gpt\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "model_path = \"yeat-gpt-final\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path).to(device)\n",
    "\n",
    "def generate_yeat_verse(prompt=\"I just pulled up\", max_new_tokens=100):\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors='pt', padding=True).to(device)\n",
    "\n",
    "    output = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        max_new_tokens=100,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        temperature=0.8,\n",
    "        repetition_penalty=1.2,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    print(generated_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3dcafe60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can't decide between the X and the Percs, bitch I'm a drug addict Yeah, fuck it, yeah, what? What happened?, where you been at for so long? Where you been in town? Go hit up that mall Oh, woo-yeah, oh, uh-uh, let's go get this shit Huh, huh, woah, wowUh, no wayI don’t got time to waitin' on nothing No wayI just came back from the dead Let's all com\n"
     ]
    }
   ],
   "source": [
    "generate_yeat_verse(\"Can't decide between\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "cf72a308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Call up Eliantte, I got diamonds on my ring Yeah, yeah, yes, diamond necklace Woo, woo, woo, fuck it, let's go Fuck it, what? Huh, no, woah, bitch, you ain't heard me call out that shit Ah-aah, ah, buh-buh-boh-bop Phew, phe—, boo-phew, pseudorеs Oh, oh-yeah, they been talkin\n"
     ]
    }
   ],
   "source": [
    "generate_yeat_verse(\"Call up\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "6ab6d2f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I just pulled up in a Tonka, I'ma blow it 'til the sun come back on Turn around then turn that bitch to an elephant Woah, woah, wooh-ooh-oop Yeah-yeah-Yeah-yeah, yeah-yeah, let's go Woo-oow, woo-oosh Buh-brah, buh-bitch, brazy, bang, oh, ah Ah-aah, hah-hae-hae-haee, hrr\n"
     ]
    }
   ],
   "source": [
    "generate_yeat_verse(\"I just pulled up in a\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yeat-gpt (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
