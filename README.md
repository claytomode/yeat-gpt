# YeatGPT

> [!CAUTION]
> **CONTENT WARNING: Vulgar and Explicit Language**
>
> This project is a technical demonstration of fine-tuning a Large Language Model (LLM) on a specific niche dataset. Because the model was trained exclusively on the lyrics of the artist **Yeat**, it will frequently generate explicit language, slang, and themes found in his music.
>
> **Disclaimer:** The outputs generated by this model are based on statistical patterns in the training data and **do not reflect my personal views, values, or professional character.** This project is just for fun using a popular musical artist.

## Overview
YeatGPT is a fine-tuned **GPT-2** model designed to mimic the unique linguistic style, rhythmic flow, and signature ad-libs of the artist Yeat. The primary goal of this project was to handle a complete "end-to-end" ML pipeline:
* **Data Engineering**: Built a custom asynchronous scraper using `httpx` to extract over 900 songs while managing strict API rate limits.
* **Data Cleaning**: Implemented complex regex and XPath selectors to scrub non-lyrical noise (e.g., "Embed" tags, contributor lists) and format the data for causal language modeling.
* **Fine-Tuning**: Trained the model on an **NVIDIA RTX 3070** using the Hugging Face Trainer API, optimizing for mixed precision (`fp16`) to achieve high-speed iterations.
* **Quantization**: Converted the final weights into **GGUF format** for efficient local execution.

## Tech Stack
* **Language**: Python 3.12+
* **Frameworks**: `transformers`, `torch`, `datasets`, `accelerate`
* **Environment Manager**: `uv`
* **Infrastructure**: Local GPU training (Ampere architecture)


  ## Current Status

  Currently, V1 is **FUNCTIONAL**

  YeatGPT is working! It is subpar at best, but I have succesfully trained the model :)
